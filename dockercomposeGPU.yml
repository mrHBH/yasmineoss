version: '3.9'
services:
  pythonbackend:
    container_name: pythonbackend
    build:
      context: ./backend/python/
      dockerfile: dockerfile.CPU
    restart: unless-stopped
    volumes:
      - ./backend/python/models/:/app/models
      - ./backend/python/src/:/app/src
      - ./tempdir/:/app/tempdir
      - ./frontend/:/app/frontend

    ports:
      - '8000:8000'
    networks:
      - yasminenet
    environment:
      - LLM_MODEL_NAME=Meta-Llama-3-8B-Instruct.Q4_0.gguf
      - LLM_MODELS_PATH=/app/models/
      - LLM_MODEL_URL=https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tsbackend:
    container_name: tsbackend
    build:
      context: ./backend/typescript/
      dockerfile: dockerfile 
    restart: unless-stopped    
    ports:
      - '8089:8089'
    networks:
      - yasminenet

  browserfrontend:
    container_name: browserfrontend
    build:
      context: ./frontend/
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - ./frontend/:/frontend
      - /frontend/node_modules
    ports:
      - '3001:3001'
    networks:
      - yasminenet

  
  ollamabackend:
    image: ollama/ollama:latest
    tty: true

    container_name: ollamabackend
    volumes:
      - ./ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    # entrypoint: 
    #       - bash
    #       - -c
    #       - |
    #         sleep 5           
    #       - ollama pull phi3:3.8b     
    networks:
      - yasminenet
     
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]
 


  # cppbackend:
  #   container_name: cppbackend
  #   build:
  #     context: ./backend/c++/
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
 
  #   ports:
  #     - '8080:8080'
  #   networks:
  #     - yasminenet


  # rustbackend:
  #   container_name: rustbackend
  #   build:      
  #     context: ./backend/rust/
  #     dockerfile: Dockerfile
  #   restart: unless-stopped 
  #   ports:
  #     - '8420:8420'
  #   networks:
  #     - yasminenet
 
networks:
  yasminenet:

volumes:
  frontend:
  tempdir: