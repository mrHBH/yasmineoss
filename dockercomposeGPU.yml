version: '3.9'
services:
  pythonbackend:
    container_name: pythonbackend
    build:
      context: ./backend/python/
      dockerfile: dockerfile.GPU 
    restart: unless-stopped
    volumes:
      - ./backend/python/models/:/app/models
      - ./backend/python/src/:/app/src
    ports:
      - '8000:8000'
    networks:
      - yasminenet
    environment:
      - LLM_MODEL_NAME=mistral-7b-instruct-v0.2.Q8_0.gguf
      - LLM_MODELS_PATH=/app/models/
      - LLM_MODEL_URL=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tsbackend:
    container_name: tsbackend
    build:
      context: ./backend/typescript/
      dockerfile: dockerfile 
    restart: unless-stopped    
    ports:
      - '8089:8089'
    networks:
      - yasminenet

  browserfrontend:
    container_name: browserfrontend
    build:
      context: ./frontend/
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - ./frontend/:/frontend
      - /frontend/node_modules
    ports:
      - '3001:3001'
    networks:
      - yasminenet


  # cppbackend:
  #   container_name: cppbackend
  #   build:
  #     context: ./backend/c++/
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
 
  #   ports:
  #     - '8080:8080'
  #   networks:
  #     - yasminenet


  # rustbackend:
  #   container_name: rustbackend
  #   build:      
  #     context: ./backend/rust/
  #     dockerfile: Dockerfile
  #   restart: unless-stopped 
  #   ports:
  #     - '8420:8420'
  #   networks:
  #     - yasminenet
 
networks:
  yasminenet:

